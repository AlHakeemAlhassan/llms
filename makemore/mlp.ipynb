{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb6151f8-7b27-47bd-81df-056c78d5ea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86c5ded7-d1bb-4d76-9f3f-050f4676b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['eliette', 'aryan', 'raeley', 'torrey', 'kaylon']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\twords = [w for w in words if w] # get rid of any empty strings\n",
    "\tnames = sorted(words, key=lambda x: random.random())\n",
    "\t\n",
    "with open(\"names.txt\", \"w\") as f: \n",
    "\tjoined = \"\\n\".join(names)\n",
    "\tf.write(joined)\n",
    "min_chars = 1\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "chars_count = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), chars_count))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ece9b2e4-7d31-4397-b570-f01f02c3e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]), torch.int64, torch.int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    context  = [0] * block_size\n",
    "    for ch in name:\n",
    "        ix = atoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "        \n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40df7547-351a-400c-81a8-7b5d5da68526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... => e\n",
      "..e => l\n",
      ".el => i\n",
      "eli => e\n",
      "lie => t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((torch.Size([22829, 3]), torch.Size([22829])),\n",
       " (torch.Size([182430, 3]), torch.Size([182430])),\n",
       " (torch.Size([22887, 3]), torch.Size([22887])))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build_dset basically builds a rolling window on the dataset based on the context length.\n",
    "def build_dset(dset):\n",
    "    X, Y = [], []\n",
    "    for name in dset:\n",
    "        context  = [0] * block_size\n",
    "        for ch in name:\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "n1 = int(0.1*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1])\n",
    "X_val, Y_val = build_dset(names[n1:n2])\n",
    "X_test, Y_test = build_dset(names[n2:])\n",
    "\n",
    "for c, d in zip(X_train[:5], Y_train[:5]):\n",
    "    print(''.join(itoa[i.item()] for i in c), \"=>\", itoa[d.item()])\n",
    "\n",
    "(X_train.shape, Y_train.shape), (X_val.shape, Y_val.shape), (X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36a4d92f-067a-4a56-a452-63c8171c6df8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2147483647\u001b[39m) \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# squeezing it into a 2 dimensional space \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# since in the paper, a relatively smaller tensor(30 and 60) (to the dataset[17_000]) is used. \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m x_enc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(X_train, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m Z \u001b[38;5;241m=\u001b[39m x_enc \u001b[38;5;241m@\u001b[39m C\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "# squeezing it into a 2 dimensional space \n",
    "# since in the paper, a relatively smaller tensor(30 and 60) (to the dataset[17_000]) is used. \n",
    "C = torch.randn((27, 2), generator=g)\n",
    "\n",
    "x_enc = F.one_hot(X_train, num_classes=27).float()\n",
    "Z = x_enc @ C\n",
    "\n",
    "# this is another way of doing x \\times W \n",
    "emb = C[X_train]\n",
    "\n",
    "assert torch.equal(Z, emb)\n",
    "print(\"The shape of Z(emb): \", Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "72b8b03a-4fac-4fe3-914c-91efd25d8eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (468710001.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[94], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    b2 = torch.randn=(27, generator=g)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "# changing shape of Z(emb) so as to cross weight \n",
    "A = torch.cat(torch.unbind(Z, 1), dim=1)\n",
    "A.shape\n",
    "\n",
    "# This is a more efficient implementation of the above\n",
    "Z_shaped = Z.view(-1, 6)\n",
    "\n",
    "assert torch.equal(A, Z_shaped)\n",
    "\n",
    "# here let's use a 100 dimensional space\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(27, generator=g)\n",
    " \n",
    "h = torch.tanh(Z_shaped @ W1 + b1) #hidden layer\n",
    "\n",
    "print(\"Shape of hidden layer: \", h.shape)\n",
    "\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn=(27, generator=g)\n",
    "\n",
    "logits = h @ W2 + b2 # log-counts\n",
    "print(\"Shape of logits: \", logits.shape)\n",
    "\n",
    "# normalization\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "print(\"Shape of probability space: \", probs.shape)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of parameters: \", sum(p.nelement() for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b9aef35-5913-44bf-94a4-8044d21029f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# negative mean log likelihood (cross entropy loss)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mprobs\u001b[49m[torch\u001b[38;5;241m.\u001b[39marange(), Y_train]\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# another way\u001b[39;00m\n\u001b[1;32m      5\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "# negative mean log likelihood (cross entropy loss)\n",
    "loss = -probs[torch.arange(), Y_train].log().mean()\n",
    "\n",
    "# this is a better way to implement the above\n",
    "loss2 = F.cross_entropy(logits, Y_train)\n",
    "\n",
    "assert torch.equal(loss, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16087e-6737-408b-ad7a-5f6e0b7a990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params:\n",
    "    p.requires_grad = True # autograd should record operations\n",
    "\n",
    "# BACKPROPAGATION\n",
    "\n",
    "\n",
    "# minibatch construct\n",
    "# forward pass\n",
    "# backward pass\n",
    "# update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6ead9-450f-4543-8043-37cc00c232ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from model\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
