{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6151f8-7b27-47bd-81df-056c78d5ea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c5ded7-d1bb-4d76-9f3f-050f4676b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['tyrie', 'kesia', 'lainey', 'avyana', 'mylan']\n",
      "number of names:  32033\n",
      "(list of chars, count):  ('.abcdefghijklmnopqrstuvwxyz', 27)\n",
      "(max word length, min word length):  (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "with open(\"names.txt\", \"r+\") as f:\n",
    "\twords = f.read().splitlines()\n",
    "\twords = [word.strip() for word in words] # get rid of any trailing spaces\n",
    "\twords = [w for w in words if w] # get rid of any empty strings\n",
    "\tnames = sorted(words, key=lambda x: random.random())\n",
    "\t\n",
    "with open(\"names.txt\", \"w\") as f: \n",
    "\tjoined = \"\\n\".join(names)\n",
    "\tf.write(joined)\n",
    "min_chars = 1\n",
    "max_chars = max(len(v) for v in names)\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "\n",
    "# in replacement of the start and end token. Every name should end with a period. and there should be no start token to begin a sequence\n",
    "chars = ['.'] + chars\n",
    "chars_count = len(chars)\n",
    "print(\"names: \", names[:5])\n",
    "print(\"number of names: \", len(names))\n",
    "print(\"(list of chars, count): \", (\"\".join(chars), chars_count))\n",
    "print(\"(max word length, min word length): \", (max_chars, min_chars))\n",
    "\n",
    "atoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itoa = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# adding end token to each name\n",
    "names = [list(name) + ['.'] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece9b2e4-7d31-4397-b570-f01f02c3e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]), torch.int64, torch.int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    context  = [0] * block_size\n",
    "    for ch in name:\n",
    "        ix = atoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "        \n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40df7547-351a-400c-81a8-7b5d5da68526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... => t\n",
      "..t => y\n",
      ".ty => r\n",
      "tyr => i\n",
      "yri => e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((torch.Size([22802, 3]), torch.Size([22802])),\n",
       " (torch.Size([182481, 3]), torch.Size([182481])),\n",
       " (torch.Size([22863, 3]), torch.Size([22863])))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build_dset basically builds a rolling window on the dataset based on the context length.\n",
    "def build_dset(dset, ctxt_len):\n",
    "    X, Y = [], []\n",
    "    for name in dset:\n",
    "        context  = [0] * ctxt_len\n",
    "        for ch in name:\n",
    "            ix = atoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itoa[i] for i in context), '--->', itoa[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "n1 = int(0.1*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1], block_size)\n",
    "X_val, Y_val = build_dset(names[n1:n2], block_size)\n",
    "X_test, Y_test = build_dset(names[n2:], block_size)\n",
    "\n",
    "for c, d in zip(X_train[:5], Y_train[:5]):\n",
    "    print(''.join(itoa[i.item()] for i in c), \"=>\", itoa[d.item()])\n",
    "\n",
    "(X_train.shape, Y_train.shape), (X_val.shape, Y_val.shape), (X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a4d92f-067a-4a56-a452-63c8171c6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Z(emb):  torch.Size([22877, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "# squeezing it into a 2 dimensional space \n",
    "# since in the paper, a relatively smaller tensor(30 and 60) (to the dataset[17_000]) is used. \n",
    "C = torch.randn((27, 2), generator=g)\n",
    "\n",
    "x_enc = F.one_hot(X_train, num_classes=27).float()\n",
    "Z = x_enc @ C\n",
    "\n",
    "# this is another way of doing x \\times W \n",
    "emb = C[X_train]\n",
    "\n",
    "assert torch.equal(Z, emb)\n",
    "print(\"The shape of Z(emb): \", Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b8b03a-4fac-4fe3-914c-91efd25d8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden layer:  torch.Size([22877, 100])\n",
      "Shape of logits:  torch.Size([22877, 27])\n",
      "Shape of probability space:  torch.Size([22877, 27])\n",
      "Total number of parameters:  3481\n"
     ]
    }
   ],
   "source": [
    "# changing shape of Z(emb) so as to cross weight \n",
    "A = torch.cat(torch.unbind(Z, 1), dim=1)\n",
    "A.shape\n",
    "\n",
    "# This is a more efficient implementation of the above\n",
    "Z_shaped = Z.view(-1, 6)\n",
    "\n",
    "assert torch.equal(A, Z_shaped)\n",
    "\n",
    "# here let's use a 100 dimensional space\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    " \n",
    "h = torch.tanh(Z_shaped @ W1 + b1) #hidden layer\n",
    "\n",
    "print(\"Shape of hidden layer: \", h.shape)\n",
    "\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "logits = h @ W2 + b2 # log-counts\n",
    "print(\"Shape of logits: \", logits.shape)\n",
    "\n",
    "# normalization\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "print(\"Shape of probability space: \", probs.shape)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of parameters: \", sum(p.nelement() for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9aef35-5913-44bf-94a4-8044d21029f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=19.509521484375 loss2=19.509521484375\n"
     ]
    }
   ],
   "source": [
    "# negative mean log likelihood (cross entropy loss)\n",
    "# print(probs.shape[0], Y_train.shape[0])\n",
    "loss = -probs[torch.arange(probs.shape[0]), Y_train].log().mean()\n",
    "\n",
    "# this is a better way to implement the above\n",
    "loss2 = F.cross_entropy(logits, Y_train)\n",
    "\n",
    "print(f\"loss={loss} loss2={loss2}\")\n",
    "# assert torch.equal(loss, loss2), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c07e7e-f847-4a4e-af0f-b26e3bfd2c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# REDOING IT AGAIN MORE CLEARELY\n",
    "# hyper-params\n",
    "embedding_size = 10\n",
    "block_size  = 3 # context length\n",
    "mid_w_size = 200 # intermediate weight size\n",
    "lre = torch.linspace(-3, 1, 100)\n",
    "lrs = 10**lre # a range of learning rate\n",
    "print(lrs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfaaecda-e11f-4d00-82fe-2673964b94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROPAGATION\n",
    "\n",
    "\n",
    "# train-test split(80, 10, 10)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train, Y_train = build_dset(names[:n1], block_size)\n",
    "X_val, Y_val = build_dset(names[n1:n2], block_size)\n",
    "X_test, Y_test = build_dset(names[n2:], block_size)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, embedding_size), generator=g)\n",
    "W1 = torch.randn((block_size * embedding_size, mid_w_size), generator=g)\n",
    "b1 = torch.randn(mid_w_size, generator=g)\n",
    "W2 = torch.randn((mid_w_size, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True # autograd should record operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4c5bfa-446d-4ada-af67-db9636a93563",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m loss_i \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(count):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m[X_train]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     14\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(emb\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, block_size \u001b[38;5;241m*\u001b[39m embedding_size) \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1) \u001b[38;5;66;03m# intermediate layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "# BACKPROPAGATION\n",
    "# minibatch construct\n",
    "# ix = torch.randint(0, X_train.shape[0], (32,))\n",
    "\n",
    "# print(emb.shape, C[X_train].shape)  \n",
    "count = 100\n",
    "step_i = []\n",
    "lr_i = []\n",
    "loss_i = []\n",
    "\n",
    "for i in range(count):\n",
    "    # forward pass\n",
    "    emb = C[X_train].view(-1, 30)\n",
    "    h = torch.tanh(emb.view(-1, block_size * embedding_size) @ W1 + b1) # intermediate layer\n",
    "    logits = h @ W2 + b2\n",
    "        \n",
    "    loss = F.cross_entropy(logits, Y_train)\n",
    "    print(\"(\", i ,\"/\", count, \") loss = \", loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data += - lrs[i] * p.grad \n",
    "\n",
    "    # track stats\n",
    "    lr_i.append(lrs[i])\n",
    "    step_i.append(i)\n",
    "    loss_i.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad1e784-b426-4518-ad31-e31536d948b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(step_i, lr_i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(step_i, lr_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
